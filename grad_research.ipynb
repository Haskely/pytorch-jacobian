{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch autograd.grad 函数研究\n",
    "\n",
    "本 notebook 将对 Pytorch 库中的 **autograd.grad** 函数行为进行详细探究。此文撰写时（2022.06.15） Pytorch 版本为 1.11.0\n",
    "\n",
    "首先查看其[官方文档](https://pytorch.org/docs/stable/generated/torch.autograd.grad.html)，翻译如下：\n",
    "\n",
    "## TORCH.AUTOGRAD.GRAD\n",
    "```python\n",
    "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False, is_grads_batched=False)\n",
    "``` \n",
    "[SOURCE 源代码](https://pytorch.org/docs/stable/_modules/torch/autograd.html#grad)\n",
    "\n",
    "> Computes and returns the sum of gradients of outputs with respect to the inputs.\n",
    "\n",
    "计算并返回 输出相对于输入的 梯度的 求和（完全搞不清楚这在说什么）\n",
    "\n",
    "> grad_outputs should be a sequence of length matching output containing the “vector” in vector-Jacobian product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn’t require_grad, then the gradient can be None).\n",
    "\n",
    "参数 “grad_outputs” 应该是一个序列，长度对应于输出，每个元素是 “向量雅可比积（vector-Jacobian product）” 中的 “向量”。此 “向量”序列 通常代表着先前计算的 关于每个输出的 梯度序列。如果一个输出没有 require_grad 标记，那么其梯度可以是 None。（同样也没看明白）\n",
    "\n",
    "NOTE 注意\n",
    "> If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.\n",
    "> 如果在用户指定的CUDA流上下文中运行任何正向运算、创建grad_outputs和/或调用grad，请参阅向后传递的流语义。（完全不知所云）\n",
    "\n",
    "NOTE 注意\n",
    "> only_inputs argument is deprecated and is ignored now (defaults to True). To accumulate gradient for other parts of the graph, please use torch.autograd.backward.\n",
    "> 参数 “only_inputs” 已弃用，现在将被忽略(默认为True)。要累加计算图中其他部分的梯度，请使用 “torch.autograd.backward”。\n",
    "\n",
    "Parameters 参数\n",
    "\n",
    "> outputs (sequence of Tensor) – outputs of the differentiated function.\n",
    "\n",
    "outputs (Tensor 的序列) – 可微函数的输出序列。\n",
    "\n",
    "> inputs (sequence of Tensor) – Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad).\n",
    "\n",
    "inputs (Tensor 的序列) – 应该被计算梯度的输入序列，这些梯度将成为函数的返回值（这些梯度不会被累加到 .grad 中去）\n",
    "\n",
    "> grad_outputs (sequence of Tensor) – The “vector” in the vector-Jacobian product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None.\n",
    "\n",
    "grad_outputs (Tensor 的序列) – “向量雅可比积（vector-Jacobian product）” 中的 “向量”。通常代表着先前计算的 关于每个输出的 梯度序列。对于标量输出或者不需要计算梯度的输出，“向量”可以为 None。 若对于所有的输出其对应的“向量”都可以是 None，那么该参数可以直接忽略，不指定. 默认值：None。\n",
    "\n",
    "> retain_graph (bool, optional) – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph.\n",
    "\n",
    "retain_graph (布尔值, 可选) – 如果为 False，则用于计算梯度的计算图所占内存将被释放。请注意，在几乎所有情况下，都不需要将此选项设置为 True，此时可以使该函数效率更高（省内存）。默认为参数 create_graph 的值。\n",
    "\n",
    "> create_graph (bool, optional) – If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False.\n",
    "\n",
    "create_graph (布尔值, 可选) – 如果为 True，则将同时构造此导数的计算图，从而允许计算更高阶导数。默认值：False。\n",
    "\n",
    "> allow_unused (bool, optional) – If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.\n",
    "\n",
    "allow_unused (布尔值, 可选) – 如果为 False，则在计算输出对应某个输入的梯度时，若该输入未在该输出的计算图中(因此它们的梯度始终为零)，就会报错。默认为 False。\n",
    "\n",
    "> is_grads_batched (bool, optional) – If True, the first dimension of each tensor in grad_outputs will be interpreted as the batch dimension. Instead of computing a single vector-Jacobian product, we compute a batch of vector-Jacobian products for each “vector” in the batch. We use the vmap prototype feature as the backend to vectorize calls to the autograd engine so that this computation can be performed in a single call. This should lead to performance improvements when compared to manually looping and performing backward multiple times. Note that due to this feature being experimental, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file an issue on github if warnings exist for your use case. Defaults to False.\n",
    "\n",
    "is_grads_batched (bool, optional) – 如果为 True，则 grad_outputs 中每个张量的第一维将被解释为批次维。我们不是计算单个向量雅可比乘积，而是为批次中的每个“向量”计算一批向量雅可比乘积。我们使用 vmap 原型功能作为后端来向量化对 autograd 引擎的调用，以便可以在单个调用中执行此计算。与手动循环和多次向后执行相比，这应该会带来性能改进。请注意，由于此功能处于实验阶段，因此可能会出现性能骤降。如果您的用例存在警告，请使用torch._C._debug_only_display_vmap_fallback_warnings(True) 显示任何性能警告，并在github上提交问题。默认为 False。\n",
    "\n",
    "看完此文档，感觉对函数用法以及每一个参数都做了解释，但是又含糊不清。看来还是需要进行进一步实验来确定其行为。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标量函数\n",
    "\n",
    "### 对 outputs 与 inputs 参数的研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(3.))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "def f(x):\n",
    "    return 2.0 * x + 1.0\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "x.requires_grad_()\n",
    "\n",
    "grad(f(x),x) # (tensor(2.),)\n",
    "grad(f(x),(x,x)) # (tensor(2.), tensor(2.))\n",
    "grad((f(x),f(x)),x) # (tensor(4.),)\n",
    "grad(f(x) + f(x),x) # (tensor(4.),)\n",
    "grad((f(x),f(x)),(x,x)) # (tensor(4.), tensor(4.))\n",
    "grad((f(x),x),(x,x)) # (tensor(3.), tensor(3.))\n",
    "grad((x,f(x)),(x,x)) # (tensor(3.), tensor(3.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察以上输出，结合源代码，做出如下猜想：\n",
    "1. 输入的 outputs 与 inputs 若不是元组，则首先会处理成一个单元素的元组\n",
    "2. 输出一个元组，且长度与输入的 inputs 参数元组长度相等，每一个输出分量对应于 inputs 每一个分量\n",
    "3. 输入的 outputs 有多个的话则梯度会累加，换句话说，程序自动把多个 outputs 相加了，再计算每个 inputs 的梯度\n",
    "\n",
    "第 3. 点引出一个问题，它会累积梯度是因为重复计算吗，那如果预先计算好会怎样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.),)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(f(x),x) # (tensor(2.),)\n",
    "y = f(x)\n",
    "# grad(y + y,x) # (tensor(4.),)\n",
    "grad((y,y),x) # (tensor(4.),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答案是依然会累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(f(x1,x2),x1): (tensor(4.5000),)\n",
      "grad(f(x1,x2),x2): (tensor(5.5000),)\n",
      "grad(f(x1,x2),(x1,x2)): (tensor(4.5000), tensor(5.5000))\n",
      "grad((f(x1,x2),x1 + x2),(x1,x2)): (tensor(5.5000), tensor(6.5000))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "def f(x1,x2):\n",
    "    return x1 * x2 + 3 * x1 + 5 * x2\n",
    "\n",
    "x1 = torch.tensor(0.5)\n",
    "x1.requires_grad_()\n",
    "x2 = torch.tensor(1.5)\n",
    "x2.requires_grad_()\n",
    "\n",
    "print('grad(f(x1,x2),x1):',grad(f(x1,x2),x1)) # (tensor(4.5000),)\n",
    "print('grad(f(x1,x2),x2):',grad(f(x1,x2),x2)) # (tensor(5.5000),)\n",
    "print('grad(f(x1,x2),(x1,x2)):',grad(f(x1,x2),(x1,x2))) # (tensor(4.5000), tensor(5.5000))\n",
    "print('grad((f(x1,x2),x1 + x2),(x1,x2)):',grad((f(x1,x2),x1 + x2),(x1,x2))) # (tensor(5.5000), tensor(6.5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出以上猜想依然成立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对 allow_unused 参数的研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(2 * x1 + 0.0 * x2,(x1,x2)): (tensor(2.), tensor(0.))\n",
      "grad(2 * x1,(x1,x2),allow_unused=True): (tensor(2.), None)\n"
     ]
    }
   ],
   "source": [
    "print('grad(2 * x1 + 0.0 * x2,(x1,x2)):',grad(2 * x1 + 0.0 * x2,(x1,x2))) # (tensor(2.), tensor(0.))\n",
    "# print('grad(2 * x1,(x1,x2)):',grad(2 * x1,(x1,x2))) 报错 One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n",
    "print('grad(2 * x1,(x1,x2),allow_unused=True):',grad(2 * x1,(x1,x2),allow_unused=True)) # (tensor(2.), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，若某一输出不在计算图中，则需要令 allow_unused=True 才不会报错，且对应的梯度输出为 None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对 grad_outputs 研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.6000),)\n",
      "(tensor(0.6000), tensor(0.6000))\n",
      "(tensor(0.6000), tensor(0.6000))\n",
      "(tensor(1.2000), tensor(1.2000))\n",
      "(tensor(1.5000), tensor(1.5000))\n",
      "(tensor(1.2000), tensor(1.2000))\n",
      "(tensor(1.2000), tensor(1.2000))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "def f(x):\n",
    "    return 2.0 * x + 1.0\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "x.requires_grad_()\n",
    "\n",
    "print(grad(f(x),x,grad_outputs=(torch.tensor(0.3),))) # (tensor(0.6000),)\n",
    "print(grad(f(x),(x,x),grad_outputs=(torch.tensor(0.3),))) # (tensor(0.6000), tensor(0.6000))\n",
    "print(grad(f(x),(x,x),grad_outputs=(torch.tensor(0.3),torch.tensor(0.6)))) # (tensor(0.6000), tensor(0.6000))\n",
    "print(grad(f(x),(x,x),grad_outputs=(torch.tensor(0.6),torch.tensor(0.3)))) # (tensor(1.2000), tensor(1.2000))\n",
    "print(grad((f(x),x),(x,x),grad_outputs=(torch.tensor(0.6),torch.tensor(0.3)))) # (tensor(1.5000), tensor(1.5000))\n",
    "print(grad((f(x),x),(x,x),grad_outputs=(torch.tensor(0.3),torch.tensor(0.6)))) # (tensor(1.2000), tensor(1.2000))\n",
    "print(grad((x,f(x)),(x,x),grad_outputs=(torch.tensor(0.6),torch.tensor(0.3)))) # (tensor(1.2000), tensor(1.2000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做出猜测，grad_outputs 对应于 outputs 的每一个元素，在计算梯度时会将对应梯度相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量函数\n",
    "\n",
    "### 对 outputs 与 inputs 参数的研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.,  7., 11.]]),)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "W = torch.tensor([[1,2],[3,4],[5,6]],dtype=torch.float) # 3 x 2\n",
    "b = torch.tensor([[7,8]],dtype=torch.float) # 1 x 2\n",
    "\n",
    "x =torch.tensor([[1.1,1.2,1.3]]) # 1 x 3\n",
    "x.requires_grad_()\n",
    "\n",
    "# grad(x @ W + b,x) # grad can be implicitly created only for scalar outputs\n",
    "grad((x @ W + b).sum(),x) # (tensor([[ 3.,  7., 11.]]),) 输出和 inputs 形状一致的 梯度向量\n",
    "grad((x @ W + b).sum(),(x,x)) # (tensor([[ 3.,  7., 11.]]), tensor([[ 3.,  7., 11.]]))\n",
    "grad(((x @ W + b).sum(),x.sum()),x) # (tensor([[ 4.,  8., 12.]]),)\n",
    "grad((x @ W + b).sum() + x.sum(),x) # (tensor([[ 4.,  8., 12.]]),)\n",
    "\n",
    "# y = (x @ W + b).sum()\n",
    "# grad(y,x) # (tensor([[ 3.,  7., 11.]]),)\n",
    "# grad((y,y),x) 3 (tensor([[ 6., 14., 22.]]),)\n",
    "y = (x @ W + b)\n",
    "# grad([y[:,0],y[:,1]],x) #(tensor([[ 3.,  7., 11.]]), tensor([[ 3.,  7., 11.]]))\n",
    "# grad([y[0,0],y[0,1]],x) # (tensor([[ 3.,  7., 11.]]),)\n",
    "# grad([y[:,0]],x) # (tensor([[1., 3., 5.]]),)\n",
    "grad(y,x,grad_outputs=(torch.tensor([[1,1]]),)) # (tensor([[ 3.,  7., 11.]]),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "got 2 tensors and 1 gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20488/3990682289.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\haske\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    276\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: got 2 tensors and 1 gradients"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "\n",
    "def f1(x:torch.Tensor):\n",
    "    W = torch.tensor([[1,2],[3,4],[5,6]],dtype=torch.float) # 3 x 2\n",
    "    b = torch.tensor([[7,8]],dtype=torch.float) # 1 x 2\n",
    "    y = x @ W + b\n",
    "    return y\n",
    "\n",
    "def f2(x:torch.Tensor):\n",
    "    W = torch.tensor([[1,2],[3,4],[5,6],[7,8]],dtype=torch.float) # 3 x 2\n",
    "    b = torch.tensor([[9,10]],dtype=torch.float) # 1 x 2\n",
    "    y = x @ W + b\n",
    "    return y\n",
    "\n",
    "x1 = torch.rand(size = ())\n",
    "x1 = torch.tensor([1.1,1.2,1.3])\n",
    "x1.requires_grad_()\n",
    "x2 = torch.tensor([2.1,2.2,2.3,2.4])\n",
    "x2.requires_grad_()\n",
    "\n",
    "y1 = f1(x1)\n",
    "y2 = f2(x2)\n",
    "\n",
    "grad((y1,y2),(x2),grad_outputs=(torch.tensor([[0.5,0.5]]),torch.tensor([[1,1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，基本上摸清了 Pytorch 中 autograd.grad 函数的行为，总结如下：\n",
    "\n",
    "函数调用样例为：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def f1(x:torch.Tensor):\n",
    "    ...\n",
    "    return y:torch.Tensor\n",
    "\n",
    "def f2(x:torch.Tensor):\n",
    "    ...\n",
    "    return y:torch.Tensor\n",
    "\n",
    "def f3(x:torch.Tensor):\n",
    "    ...\n",
    "    return y:torch.Tensor\n",
    "\n",
    "x1_shape = (1,m1)\n",
    "x2_shape = (1,m2)\n",
    "x3_shape = (1,m3)\n",
    "\n",
    "y1_shape = (1,n1)\n",
    "y2_shape = (1,n2)\n",
    "y3_shape = (1,n3)\n",
    "\n",
    "x1 = torch.rand(size = x1_shape)\n",
    "x2 = torch.rand(size = x1_shape)\n",
    "x3 = torch.rand(size = x1_shape)\n",
    "for x in (x1,x2,x3):\n",
    "    x.requires_grad_()\n",
    "\n",
    "y1 = f1(x1)\n",
    "y2 = f2(x2)\n",
    "y3 = f3(x3)\n",
    "\n",
    "v1 = torch.rand(size = y1_shape)\n",
    "v2 = torch.rand(size = y2_shape)\n",
    "v3 = torch.rand(size = y3_shape)\n",
    "\n",
    "results = torch.autograd.grad(\n",
    "    outputs = (y1,y2,y3), \n",
    "    inputs = (x1,x2), \n",
    "    grad_outputs=(v1,v2,v3), \n",
    "    retain_graph=None, \n",
    "    create_graph=False, \n",
    "    only_inputs=True, \n",
    "    allow_unused=False, \n",
    "    is_grads_batched=False\n",
    ")\n",
    "```\n",
    "\n",
    "参考以上样例，对于如下的调用：\n",
    "```python\n",
    "(g1,g2,...,gm) = torch.autograd.grad(\n",
    "    outputs = (y1,y2,...,yn), \n",
    "    inputs = (x1,x2,...,xm), \n",
    "    grad_outputs=(v1,v2,...vn), \n",
    "    retain_graph=None, \n",
    "    create_graph=False, \n",
    "    only_inputs=True, \n",
    "    allow_unused=False, \n",
    "    is_grads_batched=False\n",
    ")\n",
    "```\n",
    "要求：\n",
    "1. 任意 xi， 都需要执行 xi.requires_grad_()\n",
    "2. 任意 xi，需存在 yj，使得 xi 参与了 yj 的计算，否则需要设置 allow_unused=True 才不会报错，此时输出的对应 gi 为 None。\n",
    "3. 任意 yi，对应一个 vi，两者形状需相同。当 yi 为标量时，vi 可以为 None，此时 vi 自动取 1.0；当所有的 yi 均为标量时，grad_outputs 可以取 None（这也是其默认值），此时 grad_outputs 自动取 (1.0,1.0,...1.0) 【n个】。\n",
    "4. 函数执行一遍后若再次对 yi 求偏导会报错，因为执行一遍后会删除计算图，除非设置 retain_graph=True。\n",
    "5. 若之后想求 gi 对 xj 的偏导（也就是高阶导）会报错，因为 gi 没有 连接 xj 的计算图，除非设置 create_graph=True（此时 retain_graph=create_graph=True）\n",
    "6. only_inputs 参数没有效果，已被弃用。\n",
    "7. is_grads_batched 暂未测试。\n",
    "\n",
    "程序行为：\n",
    "函数会输出 m 个向量的元组 (g1,g2,...,gm)，其中 gi 代表 :\n",
    "$$\n",
    "    \\vec{g}_i = \\left(\\frac{\\partial \\sum_{j=1}^{n} \\vec{v}_j \\cdot \\vec{y}_j}{\\partial \\vec{x}_{i_1}}, \\cdots, \\frac{\\partial \\sum_{j=1}^{n} \\vec{v}_j \\cdot \\vec{y}_j}{\\partial \\vec{x}_{i_s}}\\right) \n",
    "$$\n",
    "文字描述一下，也就是说，首先程序会将 vi 与 yi 两两做内积然后相加，得到合并的标量输出 y，然后对每一个 xj 的每一个分量 xjk，计算 y 对 xjk 的偏导数，最后把 xjk 组装成 gj；m 个 xj 对应 m 个 gj，合并成一个元组后返回。\n",
    "\n",
    "个人总结：\n",
    "基于以上说明，我自己在使用 torch.autograd.grad() 函数时，首先 outputs 永远只输入一个标量值，同时永远忽略 grad_outputs 参数（也就是让其默认为 1.0）。因为如果有需要自己完全可以在外面做好向量内积相加等操作，为何交给这个行为不写明的函数来做呢。\n",
    "\n",
    "其次在写通用的求导工具函数时，allow_unused 永远设为 True，然后自行将 None 处理成 0，因为如果某个 xi 没有被使用，其导数数学意义就是 0，让他报错干嘛。\n",
    "\n",
    "完毕。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 3., 5.],\n",
       "         [2., 4., 6.]]),)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "def f1(x:torch.Tensor):\n",
    "    W = torch.tensor([[1,2],[3,4],[5,6]],dtype=torch.float) # 3 x 2\n",
    "    b = torch.tensor([[7,8]],dtype=torch.float) # 1 x 2\n",
    "    y = x @ W + b\n",
    "    return y\n",
    "\n",
    "def f2(x:torch.Tensor):\n",
    "    W = torch.tensor([[1,2],[3,4],[5,6],[7,8]],dtype=torch.float) # 3 x 2\n",
    "    b = torch.tensor([[9,10]],dtype=torch.float) # 1 x 2\n",
    "    y = x @ W + b\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.rand(size = ())\n",
    "x1 = torch.tensor([1.1,1.2,1.3])\n",
    "x1.requires_grad_()\n",
    "x2 = torch.tensor([2.1,2.2,2.3,2.4])\n",
    "x2.requires_grad_()\n",
    "\n",
    "y1 = f1(x1)\n",
    "y1 = y1.reshape((y1.shape[0] * y1.shape[1]))\n",
    "I = torch.eye(y1.shape[0])\n",
    "grad((y1,),(x1,),grad_outputs=(I,), is_grads_batched= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f3(x):\n",
    "    return 1*x[0] + 2*x[1] + 3\n",
    "\n",
    "x3_1 =  torch.tensor([3.1])\n",
    "x3_2 =  torch.tensor([3.2])\n",
    "x3_3 =  torch.tensor([3.3])\n",
    "for x in (x3_1,x3_2,x3_3):\n",
    "    x.requires_grad_()\n",
    "x3 = torch.concat([x3_1,x3_2,x3_3])\n",
    "grad(x3_1 + 2 * x3_2 + 3,x3,allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 4.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[2., 5.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[3., 6.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 4.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [2., 5.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [3., 6.]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = torch.tensor([[1,2],[3,4]],dtype=torch.float)\n",
    "x4.requires_grad_()\n",
    "y4 = x4 @ torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float)\n",
    "\n",
    "(J,) = grad(y4.flatten(),x4,grad_outputs = (torch.eye(torch.numel(y4)),),is_grads_batched=True)\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 4.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[2., 5.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[3., 6.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [1., 4.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [2., 5.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [3., 6.]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.resize_(y4.shape + x4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def jacobian(y:torch.Tensor,x:torch.Tensor,need_higher_grad = True) -> torch.Tensor:\n",
    "    \"\"\"基于 torch.autograd.grad 函数的更清晰明了的 API，功能是计算一个雅可比矩阵。\n",
    "\n",
    "    Args:\n",
    "        y (torch.Tensor): 函数输出向量\n",
    "        x (torch.Tensor): 函数输入向量\n",
    "        need_higher_grad (bool, optional): 是否需要计算高阶导数，如果确定不需要可以设置为 False 以节约资源. 默认为 True.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 计算好的“雅可比矩阵”。注意！输出的“雅可比矩阵”形状为 y.shape + x.shape。例如：y 是 n 个元素的张量，y.shape = [n]；x 是 m 个元素的张量，x.shape = [m]，则输出的雅可比矩阵形状为 n x m，符合常见的数学定义。\n",
    "        但是若 y 是 1 x n 的张量，y.shape = [1,n]；x 是 1 x m 的张量，x.shape = [1,m]，则输出的雅可比矩阵形状为1 x n x 1 x m，如果嫌弃多余的维度可以自行使用 Jac.squeeze() 一步到位。\n",
    "        这样设计是因为考虑到 y 是 n1 x n2 的张量； 是 m1 x m2 的张量（或者形状更复杂的张量）时，输出 n1 x n2 x m1 x m2 （或对应更复杂形状）更有直观含义，方便用户知道哪一个元素对应的是哪一个偏导。\n",
    "    \"\"\"\n",
    "    (Jac,) = torch.autograd.grad(\n",
    "        outputs = (y.flatten(),),\n",
    "        inputs = (x,),\n",
    "        grad_outputs=(torch.eye(torch.numel(y)),), \n",
    "        create_graph=need_higher_grad, \n",
    "        allow_unused=True, \n",
    "        is_grads_batched=True\n",
    "    )\n",
    "    if Jac is None:\n",
    "        Jac = torch.zeros(size = (y.shape + x.shape))\n",
    "    else:\n",
    "        Jac.resize_(size = (y.shape + x.shape))\n",
    "    return Jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x:torch.Tensor):\n",
    "    W = torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float)\n",
    "    b = torch.tensor([7,8,9],dtype=torch.float)\n",
    "    return x @ W + b \n",
    "\n",
    "x = torch.tensor([0.1,0.2])\n",
    "x.requires_grad_()\n",
    "y = f1(x)\n",
    "\n",
    "jacobian(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.1,0.2]])\n",
    "x.requires_grad_()\n",
    "y = f1(x)\n",
    "J = jacobian(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 4.]],\n",
       "\n",
       "         [[2., 5.]],\n",
       "\n",
       "         [[3., 6.]]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(0.1)\n",
    "x.requires_grad_()\n",
    "y = 2 * x + 3\n",
    "jacobian(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batched_jacobian(batched_y:torch.Tensor,batched_x:torch.Tensor,need_higher_grad = True) -> torch.Tensor:\n",
    "    \"\"\"计算一个批次的雅可比矩阵。\n",
    "        注意输入的 batched_y 与 batched_x 应该满足一一对应的关系，否则即便正常输出，其数学意义也不明。\n",
    "\n",
    "    Args:\n",
    "        batched_y (torch.Tensor): N x y_shape\n",
    "        batched_x (torch.Tensor): N x x_shape\n",
    "        need_higher_grad (bool, optional):是否需要计算高阶导数. 默认为 True.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 计算好的一个批次的雅可比矩阵张量，形状为  N x y_shape x x_shape\n",
    "    \"\"\"\n",
    "    sumed_y = batched_y.sum(dim = 0) # y_shape\n",
    "    J = jacobian(sumed_y,batched_x,need_higher_grad) # y_shape x N x x_shape\n",
    "    \n",
    "    dims = list(range(J.dim()))\n",
    "    dims[0],dims[sumed_y.dim()] = dims[sumed_y.dim()],dims[0]\n",
    "    J = J.permute(dims = dims) # N x y_shape x x_shape\n",
    "    return J\n",
    "\n",
    "def f1(x:torch.Tensor):\n",
    "    W = torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float)\n",
    "    b = torch.tensor([7,8,9],dtype=torch.float)\n",
    "    return x @ W + b \n",
    "\n",
    "batched_x = torch.rand((10,2))\n",
    "batched_x.requires_grad_()\n",
    "batched_y = f1(batched_x)\n",
    "batched_J = batched_jacobian(batched_y,batched_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]],\n",
       "\n",
       "        [[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a34dd54ed0f2b402acbfb11b83bc9ef30f9faab167f86848567b82340e586148"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
